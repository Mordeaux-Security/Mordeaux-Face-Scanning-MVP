#!/usr/bin/env python3
"""
mine-selectors - CLI tool for mining and validating image selectors

Usage:
    mine-selectors --domain example.com --urls urls.txt --out site_recipes.yaml [--append]

This tool analyzes HTML content from URLs, mines candidate selectors,
validates them, and merges successful recipes into the site_recipes.yaml file.
"""

import argparse
import asyncio
import logging
import sys
from pathlib import Path
from typing import List, Optional

# Add the project root to the path
sys.path.append(str(Path(__file__).parent.parent))

from backend.app.services.selector_mining import SelectorMiningService, emit_recipe_yaml_block, Limits, ATTR_PRIORITY, MAX_IMAGE_BYTES
from backend.app.services.http_service import create_safe_client, fetch_html_with_redirects
import httpx


def setup_logging(verbose: bool = False):
    """Set up logging configuration."""
    level = logging.DEBUG if verbose else logging.INFO
    logging.basicConfig(
        level=level,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
        datefmt='%H:%M:%S'
    )


def parse_urls_file(urls_file: str) -> List[str]:
    """
    Parse URLs from a text file.
    
    Args:
        urls_file: Path to file containing URLs (one per line)
        
    Returns:
        List of URLs
        
    Raises:
        FileNotFoundError: If the file doesn't exist
        ValueError: If no valid URLs are found
    """
    urls_path = Path(urls_file)
    if not urls_path.exists():
        raise FileNotFoundError(f"URLs file not found: {urls_file}")
    
    urls = []
    with open(urls_path, 'r', encoding='utf-8') as f:
        for line_num, line in enumerate(f, 1):
            url = line.strip()
            if url and not url.startswith('#'):  # Skip empty lines and comments
                urls.append(url)
    
    if not urls:
        raise ValueError(f"No valid URLs found in {urls_file}")
    
    logging.info(f"Loaded {len(urls)} URLs from {urls_file}")
    return urls


async def fetch_html_content(urls: List[str], max_urls: int = 3) -> Optional[str]:
    """
    Fetch HTML content from the first few URLs.
    
    Args:
        urls: List of URLs to try
        max_urls: Maximum number of URLs to try
        
    Returns:
        HTML content from the first successful fetch, or None if all fail
    """
    async with create_safe_client(timeout=30.0) as client:
        for i, url in enumerate(urls[:max_urls]):
            try:
                logging.info(f"Fetching HTML from: {url}")
                html_content, reason = await fetch_html_with_redirects(url, client, max_hops=3)
                
                if html_content is not None:
                    logging.info(f"Successfully fetched HTML from {url} ({len(html_content)} chars)")
                    return html_content
                else:
                    logging.warning(f"Failed to fetch {url}: {reason}")
                    
            except Exception as e:
                logging.warning(f"Error fetching {url}: {e}")
    
    return None


async def mine_selectors_for_domain(domain: str, urls: List[str], verbose: bool = False, js_fallback: bool = False, min_candidates: int = 3, max_bytes: Optional[int] = None) -> Optional[dict]:
    """
    Mine selectors for a domain from the provided URLs.
    
    Args:
        domain: Domain name to analyze
        urls: List of URLs to fetch HTML from
        verbose: Enable verbose logging
        js_fallback: Enable JavaScript fallback for dynamic content
        min_candidates: Minimum candidates before JS fallback
        max_bytes: Maximum image size in bytes
        
    Returns:
        Recipe dictionary if successful, None otherwise
    """
    logging.info(f"Starting selector mining for domain: {domain}")
    
    # Fetch HTML content
    html_content = await fetch_html_content(urls)
    if not html_content:
        logging.error("Failed to fetch HTML content from any URL")
        return None
    
    # Use the new SelectorMiningService
    try:
        # Create limits and mining service
        limits = Limits(max_candidates=10, max_samples_per_candidate=3, max_bytes=max_bytes or MAX_IMAGE_BYTES)
        
        # Use the new SelectorMiningService
        async with SelectorMiningService() as mining_service:
            result = await mining_service.mine_page(
                url=urls[0] if urls else f"https://{domain}",
                html=html_content,  # Use the fetched HTML
                use_js=js_fallback,
                limits=limits
            )
            
            if not result.candidates:
                logging.error("No candidates found")
                return None
            
            logging.info(f"Mining completed with status: {result.status}")
            logging.info(f"Found {len(result.candidates)} candidates")
            
            if verbose:
                logging.info("Top selectors:")
                for i, candidate in enumerate(result.candidates[:3], 1):
                    logging.info(f"  {i}. {candidate.selector} - {candidate.description}")
            
            # Extract selectors and create recipe using emit_recipe_yaml_block
            selectors = [candidate.selector for candidate in result.candidates]
            attr_priority = list(ATTR_PRIORITY)  # Use constant
            extra_sources = []  # Could be populated from result if needed
            
            recipe_dict = emit_recipe_yaml_block(domain, selectors, attr_priority, extra_sources)
            
            # Add confidence from mining result
            recipe_dict['confidence'] = result.candidates[0].score if result.candidates else 0.0
            
            return recipe_dict
            
    except Exception as e:
        logging.error(f"Error mining selectors: {e}")
        return None


def merge_recipe_to_yaml(recipe_dict: dict, yaml_file: str, append: bool = True) -> bool:
    """
    Merge recipe into YAML file.
    
    Args:
        recipe_dict: Recipe dictionary to merge
        yaml_file: Path to YAML file
        append: If True, append to existing file; if False, create new file
        
    Returns:
        True if successful, False otherwise
    """
    try:
        import yaml
        
        # Load existing recipes if append is True and file exists
        existing_recipes = {
            'schema_version': 1,
            'defaults': {
                'selectors': [
                    {'selector': '.video-thumb img', 'description': '.video-thumb container images'},
                    {'selector': '.thumbnail img', 'description': '.thumbnail container images'},
                    {'selector': '.thumb img', 'description': '.thumb container images'},
                    {'selector': 'picture img', 'description': 'picture element images'},
                    {'selector': 'img.lazy', 'description': 'lazy-loaded images'},
                    {'selector': 'img', 'description': 'all images'}
                ],
                'attributes_priority': ['data-src', 'data-srcset', 'srcset', 'src'],
                'extra_sources': [
                    "meta[property='og:image']::attr(content)",
                    "link[rel='image_src']::attr(href)",
                    "video::attr(poster)",
                    "img::attr(srcset)",
                    "source::attr(srcset)",
                    "source::attr(data-srcset)",
                    "script[type='application/ld+json']::jsonpath($.thumbnailUrl, $.image, $..thumbnailUrl, $..image, $..url)",
                    "::style(background-image)"
                ],
                'method': 'smart'
            },
            'sites': {}
        }
        yaml_path = Path(yaml_file)
        
        if append and yaml_path.exists():
            with open(yaml_path, 'r', encoding='utf-8') as f:
                loaded_recipes = yaml.safe_load(f) or {}
                # If existing file has schema v1 structure, use it
                if 'schema_version' in loaded_recipes and loaded_recipes['schema_version'] == 1:
                    existing_recipes = loaded_recipes
                else:
                    # Auto-fix old format by enforcing schema v1
                    logging.warning(f"Auto-fixed: {yaml_file} does not conform to schema v1, enforcing compliance")
                    # Preserve any existing sites but ensure schema_version is set
                    if 'sites' in loaded_recipes:
                        existing_recipes['sites'] = loaded_recipes['sites']
        
        # Ensure required sections exist
        if 'sites' not in existing_recipes:
            existing_recipes['sites'] = {}
        
        # Add the recipe
        existing_recipes['sites'][recipe_dict['domain']] = {
            'selectors': recipe_dict['selectors'],
            'attributes_priority': recipe_dict['attributes_priority'],
            'extra_sources': recipe_dict['extra_sources'],
            'method': recipe_dict['method']
        }
        
        # Ensure schema_version is always present
        existing_recipes['schema_version'] = 1
        
        # Write back to file
        with open(yaml_path, 'w', encoding='utf-8') as f:
            yaml.dump(existing_recipes, f, default_flow_style=False, sort_keys=False)
        
        logging.info(f"Successfully merged recipe for {recipe_dict['domain']} into {yaml_file}")
        return True
        
    except Exception as e:
        logging.error(f"Failed to merge recipe: {e}")
        return False


async def main():
    """Main CLI function."""
    parser = argparse.ArgumentParser(
        description="Mine and validate image selectors for a domain",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
  mine-selectors --domain example.com --urls urls.txt --out site_recipes.yaml
  mine-selectors --domain example.com --urls urls.txt --out site_recipes.yaml --append --verbose
  mine-selectors --domain example.com --urls urls.txt --out site_recipes.yaml --js --min-candidates 2
        """
    )
    
    parser.add_argument('--domain', required=True, help='Domain name to analyze')
    parser.add_argument('--urls', required=True, help='File containing URLs (one per line)')
    parser.add_argument('--out', required=True, help='Output YAML file path')
    parser.add_argument('--append', action='store_true', help='Append to existing YAML file (default: True)')
    parser.add_argument('--js', action='store_true', help='Enable JavaScript fallback for dynamic content (requires Playwright)')
    parser.add_argument('--min-candidates', type=int, default=3, help='Minimum candidates before JS fallback (default: 3)')
    parser.add_argument('--max-bytes', type=int, help='Maximum image size in bytes (overrides MINER_MAX_IMAGE_BYTES env var)')
    parser.add_argument('--verbose', '-v', action='store_true', help='Enable verbose logging')
    
    args = parser.parse_args()
    
    # Set up logging
    setup_logging(args.verbose)
    
    try:
        # Parse URLs
        urls = parse_urls_file(args.urls)
        
        # Mine selectors
        recipe_dict = await mine_selectors_for_domain(
            args.domain, 
            urls, 
            args.verbose, 
            args.js, 
            args.min_candidates,
            args.max_bytes
        )
        
        if not recipe_dict:
            logging.error("Failed to mine selectors")
            sys.exit(1)
        
        # Merge into YAML file
        success = merge_recipe_to_yaml(recipe_dict, args.out, args.append)
        
        if success:
            logging.info("âœ… Selector mining completed successfully!")
            print(f"\nRecipe for {args.domain} has been added to {args.out}")
            print(f"Confidence: {recipe_dict['confidence']:.3f}")
            print(f"Selectors: {len(recipe_dict['selectors'])}")
        else:
            logging.error("Failed to merge recipe into YAML file")
            sys.exit(1)
            
    except FileNotFoundError as e:
        logging.error(f"File not found: {e}")
        sys.exit(1)
    except ValueError as e:
        logging.error(f"Invalid input: {e}")
        sys.exit(1)
    except Exception as e:
        logging.error(f"Unexpected error: {e}")
        if args.verbose:
            import traceback
            traceback.print_exc()
        sys.exit(1)


if __name__ == "__main__":
    asyncio.run(main())
